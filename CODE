import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Importing required machine learning libraries
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.cluster import KMeans
from sklearn.metrics import r2_score, accuracy_score, confusion_matrix

sns.set(style="whitegrid")

# Loading the dataset
df = pd.read_csv("laptopData.csv")

# Dropping the extra index column if it exists
if 'Unnamed: 0' in df.columns:
    df.drop(columns=['Unnamed: 0'], inplace=True)

# Cleaning the RAM column by removing 'GB' and converting it to numbers
df['Ram'] = df['Ram'].astype(str).str.replace('GB', '').str.strip()
df['Ram'] = pd.to_numeric(df['Ram'], errors='coerce')
df['Ram'] = df['Ram'].fillna(df['Ram'].median()).astype(int)

# Cleaning the Weight column by removing 'kg' and converting it to numbers
df['Weight'] = df['Weight'].astype(str).str.replace('kg', '').str.strip()
df['Weight'] = pd.to_numeric(df['Weight'], errors='coerce')
df['Weight'] = df['Weight'].fillna(df['Weight'].median())

# Making sure the Price column is numeric and handling missing values
df['Price'] = pd.to_numeric(df['Price'], errors='coerce')
df['Price'] = df['Price'].fillna(df['Price'].median())

# Encoding categorical columns so models can understand them
le = LabelEncoder()
df['Company'] = le.fit_transform(df['Company'])
df['TypeName'] = le.fit_transform(df['TypeName'])

# Plotting price distribution to understand overall price range
plt.figure(figsize=(8, 5))
sns.histplot(df['Price'], bins=20, kde=True)
plt.title("Laptop Price Distribution")
plt.show()

# Checking how RAM affects laptop price
plt.figure(figsize=(8, 5))
sns.scatterplot(x=df['Ram'], y=df['Price'])
plt.title("RAM vs Laptop Price")
plt.show()

# Checking correlation between important features
corr = df[['Company', 'TypeName', 'Ram', 'Weight', 'Price']].corr()
plt.figure(figsize=(10, 6))
sns.heatmap(corr, annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()

# Selecting features and target variable
features = ['Company', 'TypeName', 'Ram', 'Weight']
X = df[features]
y = df['Price']

# Splitting data for regression
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Training Linear Regression model
lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

print("Linear Regression R2 Score:", r2_score(y_test, y_pred_lr))

# Visualizing how close predictions are to actual prices
plt.figure(figsize=(8, 5))
plt.scatter(y_test, y_pred_lr)
plt.plot([y.min(), y.max()], [y.min(), y.max()], color='red')
plt.title("Actual vs Predicted Price")
plt.show()

# Standardizing data before applying K-Means clustering
scaler_cluster = StandardScaler()
X_cluster = scaler_cluster.fit_transform(X)

# Using elbow method to decide the best number of clusters
wcss = []
for k in range(1, 8):
    km = KMeans(n_clusters=k, random_state=42)
    km.fit(X_cluster)
    wcss.append(km.inertia_)

plt.figure(figsize=(8, 5))
plt.plot(range(1, 8), wcss, marker='o')
plt.title("Elbow Method for K-Means")
plt.show()

# Applying K-Means with 3 clusters
kmeans = KMeans(n_clusters=3, random_state=42)
df['Cluster_Segment'] = kmeans.fit_predict(X_cluster)

# Visualizing laptop clusters
plt.figure(figsize=(8, 5))
sns.scatterplot(x=df['Ram'], y=df['Price'], hue=df['Cluster_Segment'])
plt.title("Laptop Segmentation using K-Means")
plt.show()

# Creating price categories for classification models
df['Price_Category'] = pd.qcut(df['Price'], 3, labels=['Low', 'Medium', 'High'])
y_class = df['Price_Category']

# Splitting data for classification
X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(
    X, y_class, test_size=0.2, random_state=42
)

# Scaling features because KNN works on distance
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train_c)
X_test_scaled = scaler.transform(X_test_c)

# Training Decision Tree classifier
dt = DecisionTreeClassifier(max_depth=4, random_state=42)
dt.fit(X_train_c, y_train_c)
y_pred_dt = dt.predict(X_test_c)

print("Decision Tree Accuracy:", accuracy_score(y_test_c, y_pred_dt))

# Visualizing the decision tree to understand splits
plt.figure(figsize=(15, 8))
plot_tree(dt, feature_names=features, class_names=['Low', 'Medium', 'High'], filled=True)
plt.show()

# Training KNN classifier
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train_scaled, y_train_c)
y_pred_knn = knn.predict(X_test_scaled)

print("KNN Accuracy:", accuracy_score(y_test_c, y_pred_knn))

# Training Naive Bayes classifier
nb = GaussianNB()
nb.fit(X_train_scaled, y_train_c)
y_pred_nb = nb.predict(X_test_scaled)

print("Naive Bayes Accuracy:", accuracy_score(y_test_c, y_pred_nb))

# Comparing all model performances
summary = pd.DataFrame({
    'Model': ['Linear Regression (R2)', 'Decision Tree', 'KNN', 'Naive Bayes'],
    'Score': [
        r2_score(y_test, y_pred_lr),
        accuracy_score(y_test_c, y_pred_dt),
        accuracy_score(y_test_c, y_pred_knn),
        accuracy_score(y_test_c, y_pred_nb)
    ]
})

print(summary)

# Plotting comparison of model performance
plt.figure(figsize=(8, 5))
sns.barplot(x='Score', y='Model', data=summary)
plt.title("Model Performance Comparison")
plt.show()

# Saving final predictions for reference
final_output = pd.DataFrame({
    'Actual': y_test_c.values,
    'Predicted': y_pred_dt
})
final_output.to_csv("final_predictions.csv", index=False)
